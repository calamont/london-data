{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walkthrough\n",
    "Here we will walk through the finetuning script step by step to make sure it all works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List, Dict\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import (\n",
    "    DataCollator,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "from transformers.trainer_utils import is_main_process\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "\n",
    "    Using `HfArgumentParser` we can turn this class\n",
    "    into argparse arguments to be able to specify them on\n",
    "    the command line.\n",
    "    \"\"\"\n",
    "\n",
    "    max_seq_length: int = field(\n",
    "        default=128,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "            \"than this will be truncated, sequences shorter will be padded.\"\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Overwrite the cached preprocessed datasets or not.\"},\n",
    "    )\n",
    "    pad_to_max_length: bool = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\n",
    "            \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n",
    "        },\n",
    "    )\n",
    "    local_files: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether to use local files instead of downloading from s3.\"},\n",
    "    )\n",
    "    train_file: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"A csv file containing the training data.\"}\n",
    "    )\n",
    "    predict_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"A csv file containing the data for prediction.\"},\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        default=\"t5-base\",\n",
    "        metadata={\n",
    "            \"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"\n",
    "        }\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"Pretrained tokenizer name or path if not the same as model_name\"\n",
    "        },\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"\n",
    "        },\n",
    "    )\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"\n",
    "        },\n",
    "    )\n",
    "        \n",
    "@dataclass\n",
    "class T2TDataCollator:\n",
    "    def collate_batch(self, batch: List) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Take a list of samples from a Dataset and collate them into a batch.\n",
    "        Returns:\n",
    "            A dictionary of tensors\n",
    "        \"\"\"\n",
    "        input_ids = torch.stack([example[\"input_ids\"] for example in batch])\n",
    "        lm_labels = torch.stack([example[\"target_ids\"] for example in batch])\n",
    "        lm_labels[lm_labels[:, :] == 0] = -100\n",
    "        attention_mask = torch.stack([example[\"attention_mask\"] for example in batch])\n",
    "        decoder_attention_mask = torch.stack(\n",
    "            [example[\"target_attention_mask\"] for example in batch]\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"lm_labels\": lm_labels,\n",
    "            \"decoder_attention_mask\": decoder_attention_mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv = ['foo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    training_args = TrainingArguments(output_dir=\"results\")\n",
    "    parser = HfArgumentParser(\n",
    "        (ModelArguments, DataTrainingArguments)\n",
    "    )\n",
    "    \n",
    "    print(training_args)\n",
    "\n",
    "    model_args, data_args = parser.parse_args_into_dataclasses()\n",
    "\n",
    "    if (\n",
    "        os.path.exists(training_args.output_dir)\n",
    "        and os.listdir(training_args.output_dir)\n",
    "        and training_args.do_train\n",
    "        and not training_args.overwrite_output_dir\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "            \"Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO\n",
    "        if is_main_process(training_args.local_rank)\n",
    "        else logging.WARN,\n",
    "    )\n",
    "\n",
    "    # Log on each process the small summary:\n",
    "    logger.warning(\n",
    "        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    "    )\n",
    "    # Set the verbosity to info of the Transformers logger (on main process only):\n",
    "    if is_main_process(training_args.local_rank):\n",
    "        transformers.utils.logging.set_verbosity_info()\n",
    "        transformers.utils.logging.enable_default_handler()\n",
    "        transformers.utils.logging.enable_explicit_format()\n",
    "    logger.info(f\"Training/evaluation parameters {training_args}\")\n",
    "\n",
    "    # Set seed before initializing model.\n",
    "    set_seed(training_args.seed)\n",
    "    \n",
    "    if data_args.train_file is not None:\n",
    "        # Loading a dataset from local csv files\n",
    "        # TODO: perhaps have a `load_klydo_dataset` here that accepts a bucket\n",
    "        # and key and downloads data from s3, then loads it with `load_datastet`.\n",
    "        datasets = load_dataset(\"csv\", data_files={\"train\": data_args.train_file})\n",
    "    # Currently cannot do training and prediction in the same run\n",
    "    elif data_args.predict_file is not None:\n",
    "        datasets = load_dataset(\"csv\", data_files={\"test\": data_args.predict_file})\n",
    "    else:\n",
    "        logger.warning(\"No train or test file set. Exiting script.\")\n",
    "        return None\n",
    "    \n",
    "    config = AutoConfig.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "    )\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\n",
    "        model_args.base_model\n",
    "        if model_args.base_model\n",
    "        else model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        use_fast=model_args.use_fast_tokenizer,\n",
    "    )\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\n",
    "        model_args.model_name_or_path, config=config, cache_dir=model_args.cache_dir,\n",
    "    )\n",
    "    \n",
    "    # Padding strategy\n",
    "    if data_args.pad_to_max_length:\n",
    "        padding = \"max_length\"\n",
    "        max_length = data_args.max_seq_length\n",
    "    else:\n",
    "        # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n",
    "        padding = False\n",
    "        max_length = None\n",
    "        \n",
    "    def preprocess_function(examples):\n",
    "        input_encodings = tokenizer.batch_encode_plus(\n",
    "            examples[\"input_text\"], padding=padding, max_length=128\n",
    "        )\n",
    "        target_encodings = tokenizer.batch_encode_plus(\n",
    "            examples[\"target_text\"], padding=padding, max_length=max_length\n",
    "        )\n",
    "\n",
    "        encodings = {\n",
    "            \"input_ids\": input_encodings[\"input_ids\"],\n",
    "            \"attention_mask\": input_encodings[\"attention_mask\"],\n",
    "            \"target_ids\": target_encodings[\"input_ids\"],\n",
    "            \"target_attention_mask\": target_encodings[\"attention_mask\"],\n",
    "        }\n",
    "\n",
    "        return encodings\n",
    "\n",
    "    datasets = datasets.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        load_from_cache_file=not data_args.overwrite_cache,\n",
    "    )\n",
    "    \n",
    "    if training_args.do_train:\n",
    "        train_dataset = datasets[\"train\"]\n",
    "    else:\n",
    "        train_dataset = None\n",
    "        \n",
    "    def compute_metrics(p: EvalPrediction):\n",
    "        preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "        labels = p.label_ids\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            labels, preds, average=\"binary\"\n",
    "        )\n",
    "        acc = accuracy_score(labels, preds)\n",
    "        return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "    \n",
    "    # Initialize our Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=None,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=T2TDataCollator(),\n",
    "        prediction_loss_only=True,  # TODO: WOWTD\n",
    "    )\n",
    "    \n",
    "    # Training\n",
    "    if training_args.do_train:\n",
    "\n",
    "        # Log a few random samples from the training set:\n",
    "        for index in random.sample(range(len(train_dataset)), 3):\n",
    "            logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
    "\n",
    "        trainer.train(\n",
    "            model_path=model_args.model_name_or_path\n",
    "            if os.path.isdir(model_args.model_name_or_path)\n",
    "            else None\n",
    "        )\n",
    "        trainer.save_model()  # Saves the tokenizer too for easy upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|training_args.py:453] 2020-12-30 20:12:07,564 >> PyTorch: setting up devices\n",
      "12/30/2020 20:12:07 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
      "12/30/2020 20:12:07 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='results', overwrite_output_dir=False, do_train=False, do_eval=None, do_predict=False, model_parallel=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Dec30_20-12-07_Callums-MBP', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='results', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fp16_backend='auto', sharded_ddp=False)\n",
      "12/30/2020 20:12:07 - WARNING - __main__ -   No train or test file set. Exiting script.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments(output_dir='results', overwrite_output_dir=False, do_train=False, do_eval=None, do_predict=False, model_parallel=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Dec30_20-12-07_Callums-MBP', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='results', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fp16_backend='auto', sharded_ddp=False)\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
